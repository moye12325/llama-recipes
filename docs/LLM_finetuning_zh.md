## Llama 2 微调

在这里，我们将讨论使用几种不同方法对 Llama 2 进行微调的情况。我们将涵盖两种场景：

## 1. **参数高效模型微调**

这有助于使微调过程在1个消费级GPU上更加实惠。这些方法使我们可以将整个模型冻结，只向模型中添加微小的可学习参数/层。这样，我们只训练一小部分参数。在这个类别中最著名的方法是 [LORA](https://arxiv.org/pdf/2106.09685.pdf)、LLaMA Adapter 和 Prefix-tuning。

这些方法解决了以下三个方面的问题：

- **全模型微调的成本** - 这些方法只训练少量的额外参数，而不是整个模型，这使得它们可以在消费级GPU上运行。

- **部署成本** - 对于每个微调后的下游模型，我们需要部署一个单独的模型；然而，使用这些方法时，只需要预训练模型的一小部分参数（几 MB 而不是几 GB），就可以完成工作。在这种情况下，对于每个任务，我们只需在预训练模型之上添加这些额外的参数，因此预训练模型可以看作是骨干，这些参数则是不同任务上的头部。

- **灾难性遗忘** - 这些方法还有助于避免遗忘微调中可能发生的第一个任务。

HF [PEFT](https://github.com/huggingface/peft) 库提供了一种使用这些方法的简便方式，我们在这里使用了它。请在[这里](https://huggingface.co/blog/peft)阅读更多信息。

## 2. **全/部分参数微调**

全参数微调有其自身的优势，在这种方法中有多种策略可以帮助：

- 保持预训练模型冻结，只微调任务头，例如分类器模型。

- 保持预训练模型冻结，并在顶部添加几个全连接层。

- 在所有层上进行微调。

您还可以保持大多数层冻结，并仅微调几层。有许多不同的技术可供选择，根据不同的标准来冻结/解冻层。

在这种情况下，根据模型大小，您可能需要超过一个 GPU，特别是如果您的模型不能在一个 GPU 中进行训练。在这种情况下，Llama 2 7B 参数将无法适应一个 GPU。
您需要考虑的方式是，您需要足够的 GPU 内存来存储模型参数、梯度和优化器状态。其中每个元素，取决于您正在训练的精度，可能会占用多个参数数量 x 精度的空间（取决于它是 fp32/4 字节、fp16/2 字节还是 bf16/2 字节）。
例如，AdamW 优化器为每个参数保留 2 个参数，并且在许多情况下，这些参数是以 fp32 保存的。这意味着，根据您要训练/解冻的层数，您的 GPU 内存可能会超出一个 GPU。

**FSDP（完全分片数据并行）**

PyTorch 提供了 FSDP 包，用于训练不能适应一个 GPU 的模型。FSDP 可以让您在相同数量的资源下训练更大的模型。在 FSDP 之前，是 DDP（分布式数据并行），其中每个 GPU 持有模型的完整副本，并且只在数据上进行分片。在反向传递结束时，它会同步梯度。

FSDP 扩展了这个想法，不仅在数据上进行分片，还在模型参数、梯度和优化器状态上进行分片。这意味着每个 GPU 仅保留模型的一个分片。这将导致巨大的内存节省，使我们能够将一个更大的模型适应到相同数量的 GPU 中。例如，在 DDP 中，您最多可以将一个 7GB 内存的 GPU 中适应约 7 亿个参数的模型。假设您有 4 个 GPU，在这种情况下，即使您使用了 4 个 GPU，您仍然无法扩展到可以适应一个 GPU 中的模型大小。然而，使用 FSDP，您可以将一个 30 亿个参数的模型适应到 4 个 GPU 中，大约是原来模型大小的 4 倍。

请在这里阅读有关 FSDP 的更多信息。

为了提高 FSDP 下微调的性能，我们可以使用许多功能，例如：

- **混合精度**，在 FSDP 中比 Autocast 更灵活。它允许用户对模型参数、缓冲区和梯度设置精度。

- **激活检查点**，这是一种通过在前向传递中丢弃中间激活状态而不是将其保存在内存中的技术来节省内存。FSDP 激活检查点是分片感知的，这意味着我们需要在使用 FSDP 包装模型后应用它。在我们的脚本中，我们使用了这个功能。

- **auto_wrap_policy**，这是指定 FSDP 如何分区模型的方式，有默认支持的 transformer wrapping 策略。这使得 FSDP 可以根据

模型中的 transformer 类形成每个 FSDP 单元（模型的分区）。要识别模型中的这一层，您需要查看包装 attention 层和 MLP 的层。这有助于 FSDP 拥有更细粒度的单元，用于优化通信成本。